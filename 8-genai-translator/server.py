# ğŸ”¹ 1. Importing Required Libraries
from fastapi import FastAPI
# âœ”ï¸ FastAPI is a Python framework used to build REST APIs.

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
# âœ”ï¸ These are from LangChain core:
# â€¢	ChatPromptTemplate â†’ Used to define how we talk to the model.
# â€¢	StrOutputParser â†’ Helps convert the model's response to clean string output.

from langchain_groq import ChatGroq
# âœ”ï¸ This imports the Groq model class from LangChain's Groq integration.

import os
from dotenv import load_dotenv
# âœ”ï¸ These help you load environment variables (like API keys) from a .env file.

from langserve import add_routes
# âœ”ï¸ langserve lets you convert your LangChain chain into a REST API route.

# ğŸ”¹ 2. Load Environment Variables
load_dotenv()
groq_api_key = os.getenv("GROQ_API_KEY")
# âœ”ï¸ Loads .env file and gets the Groq API key from there.
# ğŸ§  You should have a .env file with:
# GROQ_API_KEY=your_api_key_here

# ğŸ”¹ 3. Initialize the LLM Model
model = ChatGroq(
    model_name="llama-3.1-8b-instant",
    groq_api_key=groq_api_key
)
# âœ”ï¸ This line connects to the Gemma 2-9b model using your Groq API key.

# ğŸ”¹ 4. Create a Prompt Template
system_template = "Translate the following into {language}:"
prompt_template = ChatPromptTemplate.from_messages([
    ('system', system_template),
    ('user', '{text}')
])
# ğŸ§  What's happening here?
# âœ”ï¸ You're telling the model what to do:
# â€¢	System message gives the instruction (e.g., "Translate into French")
# â€¢	User message gives the actual input text (e.g., "Hello")
# So, this works like:
# System: "Translate the following into French:"
# User: "Hello"


# ğŸ”¹ 5. Create an Output Parser
parser = StrOutputParser()
# âœ”ï¸ Converts the model's output (which might include extra formatting) into a clean string.

# ğŸ”¹ 6. Create the LangChain Chain
chain = prompt_template | model | parser
# âœ”ï¸ This combines everything into a chain:
# 1.	Prompt is built using your input
# 2.	Model gives response
# 3.	Output is parsed to clean string

# ğŸ”¹ 7. Create the FastAPI App
app = FastAPI(
    title="Langchain Server",
    version="1.0",
    description="A simple API server using Langchain runnable interfaces"
)
# âœ”ï¸ This creates your API app with a name, version, and description.

# ğŸ”¹ 8. Add the Chain as an API Route
add_routes(
    app,
    chain,
    path="/chain"
)
# âœ”ï¸ This line turns your chain into an API endpoint at:
# /chain
# Which means:
# â€¢	You can send requests to /chain/invoke to get translations.
# â€¢	FastAPI automatically builds API docs at /docs.

# ğŸ”¹ 9. Run the Server
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8000)
# âœ”ï¸ Starts the API server locally:
# â€¢	Address: http://127.0.0.1:8000
# â€¢	Docs: http://127.0.0.1:8000/docs
# ğŸ§  This allows you to test your API in browser or Postman.











