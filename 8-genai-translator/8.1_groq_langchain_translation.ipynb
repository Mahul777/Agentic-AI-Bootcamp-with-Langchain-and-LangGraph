{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d3abc6b",
   "metadata": {},
   "source": [
    "### ğŸ“Œ What is LCEL?\n",
    "\n",
    "ğŸ”¹ LCEL = LangChain Expression Language\n",
    "\n",
    "ğŸ”¹ It helps you to chain different components together in a flexible way\n",
    "\n",
    "ğŸ”¹ Useful for building complex LLM workflows step-by-step\n",
    "\n",
    "ğŸ’¡ Use Case in This Video\n",
    "\n",
    "â¡ï¸ A simple app that:\n",
    "\n",
    "ğŸ—£ï¸ Translates English text into other languages\n",
    "\n",
    "ğŸ§± Uses:\n",
    "\n",
    "â€¢\tOne LLM block\n",
    "\n",
    "â€¢\tPrompting\n",
    "\n",
    "â€¢\tSimple logic, easy for beginners\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710240ac",
   "metadata": {},
   "source": [
    "###ğŸŒ What is Groq?\n",
    "ğŸ”¹ Groq = Platform that hosts open-source LLMs and allows fast inferencing\n",
    "\n",
    "ğŸ”¹ Uses its own LPUs (Language Processing Units)\n",
    "\n",
    "ğŸ”¹ Faster than GPUs because it avoids:\n",
    "â€¢\tğŸ§  Compute density bottlenecks\n",
    "â€¢\tğŸ§  Memory bandwidth issues\n",
    "\n",
    "ğŸ”¹ LPUs allow faster and cheaper generation of text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be685598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Bonjour Comment Ã§a va ?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 47, 'total_tokens': 53, 'completion_time': 0.012101573, 'prompt_time': 0.028361229, 'queue_time': 0.050005841, 'total_time': 0.040462802}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_c40956ddc4', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--4da4af20-60b1-40bb-bd36-0a9f5d15da80-0', usage_metadata={'input_tokens': 47, 'output_tokens': 6, 'total_tokens': 53})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ğŸ“¦ 1. Load Environment Variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# ğŸ¤– 2. Load the Groq LLM Model\n",
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(\n",
    "    model_name=\"llama-3.1-8b-instant\",\n",
    "    groq_api_key=groq_api_key\n",
    ")\n",
    "\n",
    "# client=<groq.resources.chat.completions.Completions object at 0x0000021F48B41130> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000021F48AFFEC0> model_name='gemma-2-9b-it' model_kwargs={} groq_api_key=SecretStr('**********')\n",
    "\n",
    "# ğŸ§  What this does:\n",
    "# â€¢\tTells LangChain to use Gemma 2 (9B) model hosted by Groq\n",
    "# â€¢\tChatGroq is like a wrapper that makes it easy to use Groq models with LangChain\n",
    "# ğŸ”§ model=\"Gemma2-9b-It\": You choose which model to use (must be exactly as supported by Groq)\n",
    "\n",
    "# ğŸ’¬ 3. Send a Simple Message to the Model\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from English to French\"),\n",
    "    HumanMessage(content=\"Hello How are you?\")\n",
    "]\n",
    "\n",
    "result = model.invoke(messages)\n",
    "result\n",
    "# AIMessage(content='Bonjour Comment Ã§a va ?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 47, 'total_tokens': 53, 'completion_time': 0.012101573, 'prompt_time': 0.028361229, 'queue_time': 0.050005841, 'total_time': 0.040462802}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_c40956ddc4', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--4da4af20-60b1-40bb-bd36-0a9f5d15da80-0', usage_metadata={'input_tokens': 47, 'output_tokens': 6, 'total_tokens': 53})\n",
    "\n",
    "# ğŸ§  What this does:\n",
    "# â€¢\tYou define a \"conversation\" using SystemMessage and HumanMessage\n",
    "# o\tSystemMessage: tells the model what to do (instruction)\n",
    "# o\tHumanMessage: the input from the user\n",
    "# â€¢\tmodel.invoke(messages) sends this to the LLM and gets a result\n",
    "\n",
    "\n",
    "\n",
    "# ğŸ§¾ 4. Extract Plain Text From Model's Output\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "parser.invoke(result)\n",
    "# 'Bonjour Comment Ã§a va ?'\n",
    "\n",
    "# ğŸ§  What this does:\n",
    "# â€¢\tTakes the response from the LLM (which is an object)\n",
    "# â€¢\tExtracts the text part so it's easy to display or use\n",
    "# â€¢\tFor example: \"Bonjour, comment Ã§a va ?\"\n",
    "\n",
    "# ğŸ”— 5. Chain Model + Parser using LCEL\n",
    "chain = model | parser\n",
    "chain.invoke(messages)\n",
    "# 'Bonjour Comment allez-vous ?'\n",
    "\n",
    "\n",
    "# ğŸ§  What this does:\n",
    "# â€¢\tChains together the model and parser in one line\n",
    "# â€¢\tThis is called LCEL (LangChain Expression Language)\n",
    "# â€¢\tSo instead of calling them separately, you can do:\n",
    "# o\tinput â†’ model â†’ parser â†’ final text\n",
    "\n",
    "# ğŸ§  6. Create a Prompt Template (Reusable)\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "generic_template = \"Translate the following into {language}:\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", generic_template),\n",
    "    (\"user\", \"{text}\")\n",
    "])\n",
    "# ğŸ§  What this does:\n",
    "# â€¢\tDefines a prompt with placeholders\n",
    "# o\t{language} â€“ which language to translate to\n",
    "# o\t{text} â€“ the sentence to translate\n",
    "# â€¢\tNow you can reuse this template with different inputs\n",
    "# âš ï¸ Typo: \"Trnaslate\" â†’ \"Translate\" (fix that!)\n",
    "\n",
    "# ğŸ§ª 7. Use the Prompt With Some Values\n",
    "result = prompt.invoke({\"language\": \"French\", \"text\": \"Hello\"})\n",
    "result.to_messages()\n",
    "# [SystemMessage(content='Translate the following into French:', additional_kwargs={}, response_metadata={}),\n",
    "#  HumanMessage(content='Hello', additional_kwargs={}, response_metadata={})]\n",
    "\n",
    "# ğŸ§  What this does:\n",
    "# â€¢\tFills the prompt with:\n",
    "# o\tlanguage = \"French\"\n",
    "# o\ttext = \"Hello\"\n",
    "# â€¢\tConverts it into the actual message format used by the model\n",
    "\n",
    "# ğŸ” 8. Chain Everything Together with LCEL\n",
    "chain = prompt | model | parser\n",
    "chain.invoke({\"language\": \"French\", \"text\": \"Hello\"})\n",
    "# 'Bonjour.'\n",
    "# ğŸ§  This is the final magic step:\n",
    "# â€¢\tYou create one single flow:\n",
    "# 1.\tğŸ§  Prompt gets filled (prompt)\n",
    "# 2.\tğŸ¤– Sent to the model (model)\n",
    "# 3.\tâœ‚ï¸ Extracts just the text (parser)\n",
    "# ğŸ’¡ In simple words:\n",
    "# \"Take this English text, create a prompt, send to LLM, extract translation.\"\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
