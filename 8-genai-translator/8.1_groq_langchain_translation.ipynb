{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d3abc6b",
   "metadata": {},
   "source": [
    "### 📌 What is LCEL?\n",
    "\n",
    "🔹 LCEL = LangChain Expression Language\n",
    "\n",
    "🔹 It helps you to chain different components together in a flexible way\n",
    "\n",
    "🔹 Useful for building complex LLM workflows step-by-step\n",
    "\n",
    "💡 Use Case in This Video\n",
    "\n",
    "➡️ A simple app that:\n",
    "\n",
    "🗣️ Translates English text into other languages\n",
    "\n",
    "🧱 Uses:\n",
    "\n",
    "•\tOne LLM block\n",
    "\n",
    "•\tPrompting\n",
    "\n",
    "•\tSimple logic, easy for beginners\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710240ac",
   "metadata": {},
   "source": [
    "###🌐 What is Groq?\n",
    "🔹 Groq = Platform that hosts open-source LLMs and allows fast inferencing\n",
    "\n",
    "🔹 Uses its own LPUs (Language Processing Units)\n",
    "\n",
    "🔹 Faster than GPUs because it avoids:\n",
    "•\t🧠 Compute density bottlenecks\n",
    "•\t🧠 Memory bandwidth issues\n",
    "\n",
    "🔹 LPUs allow faster and cheaper generation of text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be685598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Bonjour Comment ça va ?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 47, 'total_tokens': 53, 'completion_time': 0.012101573, 'prompt_time': 0.028361229, 'queue_time': 0.050005841, 'total_time': 0.040462802}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_c40956ddc4', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--4da4af20-60b1-40bb-bd36-0a9f5d15da80-0', usage_metadata={'input_tokens': 47, 'output_tokens': 6, 'total_tokens': 53})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 📦 1. Load Environment Variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# 🤖 2. Load the Groq LLM Model\n",
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(\n",
    "    model_name=\"llama-3.1-8b-instant\",\n",
    "    groq_api_key=groq_api_key\n",
    ")\n",
    "\n",
    "# client=<groq.resources.chat.completions.Completions object at 0x0000021F48B41130> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000021F48AFFEC0> model_name='gemma-2-9b-it' model_kwargs={} groq_api_key=SecretStr('**********')\n",
    "\n",
    "# 🧠 What this does:\n",
    "# •\tTells LangChain to use Gemma 2 (9B) model hosted by Groq\n",
    "# •\tChatGroq is like a wrapper that makes it easy to use Groq models with LangChain\n",
    "# 🔧 model=\"Gemma2-9b-It\": You choose which model to use (must be exactly as supported by Groq)\n",
    "\n",
    "# 💬 3. Send a Simple Message to the Model\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from English to French\"),\n",
    "    HumanMessage(content=\"Hello How are you?\")\n",
    "]\n",
    "\n",
    "result = model.invoke(messages)\n",
    "result\n",
    "# AIMessage(content='Bonjour Comment ça va ?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 47, 'total_tokens': 53, 'completion_time': 0.012101573, 'prompt_time': 0.028361229, 'queue_time': 0.050005841, 'total_time': 0.040462802}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_c40956ddc4', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--4da4af20-60b1-40bb-bd36-0a9f5d15da80-0', usage_metadata={'input_tokens': 47, 'output_tokens': 6, 'total_tokens': 53})\n",
    "\n",
    "# 🧠 What this does:\n",
    "# •\tYou define a \"conversation\" using SystemMessage and HumanMessage\n",
    "# o\tSystemMessage: tells the model what to do (instruction)\n",
    "# o\tHumanMessage: the input from the user\n",
    "# •\tmodel.invoke(messages) sends this to the LLM and gets a result\n",
    "\n",
    "\n",
    "\n",
    "# 🧾 4. Extract Plain Text From Model's Output\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "parser.invoke(result)\n",
    "# 'Bonjour Comment ça va ?'\n",
    "\n",
    "# 🧠 What this does:\n",
    "# •\tTakes the response from the LLM (which is an object)\n",
    "# •\tExtracts the text part so it's easy to display or use\n",
    "# •\tFor example: \"Bonjour, comment ça va ?\"\n",
    "\n",
    "# 🔗 5. Chain Model + Parser using LCEL\n",
    "chain = model | parser\n",
    "chain.invoke(messages)\n",
    "# 'Bonjour Comment allez-vous ?'\n",
    "\n",
    "\n",
    "# 🧠 What this does:\n",
    "# •\tChains together the model and parser in one line\n",
    "# •\tThis is called LCEL (LangChain Expression Language)\n",
    "# •\tSo instead of calling them separately, you can do:\n",
    "# o\tinput → model → parser → final text\n",
    "\n",
    "# 🧠 6. Create a Prompt Template (Reusable)\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "generic_template = \"Translate the following into {language}:\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", generic_template),\n",
    "    (\"user\", \"{text}\")\n",
    "])\n",
    "# 🧠 What this does:\n",
    "# •\tDefines a prompt with placeholders\n",
    "# o\t{language} – which language to translate to\n",
    "# o\t{text} – the sentence to translate\n",
    "# •\tNow you can reuse this template with different inputs\n",
    "# ⚠️ Typo: \"Trnaslate\" → \"Translate\" (fix that!)\n",
    "\n",
    "# 🧪 7. Use the Prompt With Some Values\n",
    "result = prompt.invoke({\"language\": \"French\", \"text\": \"Hello\"})\n",
    "result.to_messages()\n",
    "# [SystemMessage(content='Translate the following into French:', additional_kwargs={}, response_metadata={}),\n",
    "#  HumanMessage(content='Hello', additional_kwargs={}, response_metadata={})]\n",
    "\n",
    "# 🧠 What this does:\n",
    "# •\tFills the prompt with:\n",
    "# o\tlanguage = \"French\"\n",
    "# o\ttext = \"Hello\"\n",
    "# •\tConverts it into the actual message format used by the model\n",
    "\n",
    "# 🔁 8. Chain Everything Together with LCEL\n",
    "chain = prompt | model | parser\n",
    "chain.invoke({\"language\": \"French\", \"text\": \"Hello\"})\n",
    "# 'Bonjour.'\n",
    "# 🧠 This is the final magic step:\n",
    "# •\tYou create one single flow:\n",
    "# 1.\t🧠 Prompt gets filled (prompt)\n",
    "# 2.\t🤖 Sent to the model (model)\n",
    "# 3.\t✂️ Extracts just the text (parser)\n",
    "# 💡 In simple words:\n",
    "# \"Take this English text, create a prompt, send to LLM, extract translation.\"\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
