{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dda48e8d",
   "metadata": {},
   "source": [
    "###üåê Use Case Overview\n",
    "\n",
    "‚û§ We are taking some content from a website (example: LangSmith docs)\n",
    "\n",
    "‚û§ The goal is to scrape the webpage content, split it into smaller parts, convert it into vectors, and use it for Q&A with an LLM\n",
    "\n",
    "‚û§ This is done using LangChain, OpenAI, and some additional tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112e164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è 1. Setup & Environment Variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAINAPI_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "# ‚úÖ What's happening?\n",
    "# ‚Ä¢\tYou're loading secret keys (API keys) from a .env file.\n",
    "# ‚Ä¢\tThis lets your code talk to services like OpenAI and LangChain.\n",
    "\n",
    "# üåê 2. Load Website Data\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://python.langchain.com/v0.1/docs/langsmith/\")\n",
    "docs = loader.load()\n",
    "# ‚úÖ What's happening?\n",
    "# ‚Ä¢\tYou're loading the website data.\n",
    "# ‚Ä¢\tThis is a list of all the pages on the LangChain website.\n",
    "# ‚Ä¢\tdocs now contains the text from the page.\n",
    "\n",
    "# üìÑ 3. Split the Webpage into Chunks\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "documents\n",
    "# [Document(metadata={'source': 'https://www.msn.com/en-in/money/news/petrol-diesel-fresh-prices-announced-check-rates-in-your-city-on-september-4/ar-AA1LPdK9?ocid=msedgntp&pc=DCTS&cvid=9efbc1097375464f9313f8964af0189c&ei=57', 'title': 'MSN', 'language': 'en-in'}, page_content='MSN')]\n",
    "\n",
    "# ‚úÖ What's happening?\n",
    "# ‚Ä¢\tLarge documents are hard for the AI to process.\n",
    "# ‚Ä¢\tSo, you're breaking them into smaller chunks (1000 characters with 200 characters overlap).\n",
    "# ‚Ä¢\tThis makes it easier to work with.\n",
    "\n",
    "# ü§ñ 4. Convert Text Chunks into Vectors\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "# C:\\Users\\sahus\\AppData\\Local\\Temp\\ipykernel_2812\\1129778531.py:7: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
    "#   embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstoredb = FAISS.from_documents(documents, embeddings)\n",
    "vectorstoredb\n",
    "# <langchain_community.vectorstores.faiss.FAISS at 0x2367b91dd30>\n",
    "# ‚úÖ What's happening?\n",
    "# ‚Ä¢\tThe text chunks are converted into embeddings (mathematical representations).\n",
    "# ‚Ä¢\tThese are stored in a vector database (FAISS), so you can search them later using questions.\n",
    "\n",
    "\n",
    "# ‚ùì 5. Test a Search Query\n",
    "query = \"Key Factors Influencing Fuel Prices in India\"\n",
    "result = vectorstoredb.similarity_search(query)\n",
    "result\n",
    "# [Document(id='2af89cbd-4acb-4d5a-9338-ca1a6a0b9694', metadata={'source': 'https://www.msn.com/en-in/money/news/petrol-diesel-fresh-prices-announced-check-rates-in-your-city-on-september-4/ar-AA1LPdK9?ocid=msedgntp&pc=DCTS&cvid=9efbc1097375464f9313f8964af0189c&ei=57', 'title': 'MSN', 'language': 'en-in'}, page_content='MSN')]\n",
    "# ‚úÖ What's happening?\n",
    "# ‚Ä¢\tYou ask a natural language question.\n",
    "# ‚Ä¢\tThe system finds the most similar document chunk from the vector DB.\n",
    "\n",
    "# üí¨ 6. Initialize the LLM (GPT-4o)\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "llm = ChatOllama(model=\"gemma3:1b\")\n",
    "print(llm)\n",
    "# model='gemma3:1b'\n",
    "# C:\\Users\\sahus\\AppData\\Local\\Temp\\ipykernel_2812\\3204733895.py:2: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
    "#   llm = ChatOllama(model=\"gemma3:1b\")\n",
    "\n",
    "# ‚úÖ What's happening?\n",
    "# ‚Ä¢\tYou‚Äôre telling LangChain to use OpenAI's GPT-4o as the brain to answer questions.\n",
    "\n",
    "# üîó 7. Create a Prompt Template and Document Chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\")\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "document_chain\n",
    "# RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
    "#   context: RunnableLambda(format_docs)\n",
    "# }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
    "# | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n'), additional_kwargs={})])\n",
    "# | ChatOllama(model='gemma3:1b')\n",
    "# | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
    "\n",
    "\n",
    "# ‚úÖ What's happening?\n",
    "# ‚Ä¢\tYou make a custom prompt telling GPT to only use the context provided (not guess).\n",
    "# ‚Ä¢\tThe document chain connects the prompt and the LLM\n",
    "\n",
    "# üîÑ 9. Turn the Vector DB into a Retriever\n",
    "# Retriever is a interface when user asked question it will retrieve relevant document from vector db\n",
    "# and pass to llm\n",
    "retriever = vectorstoredb.as_retriever()\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "retrieval_chain\n",
    "# RunnableBinding(bound=RunnableAssign(mapper={\n",
    "#   context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
    "#            | VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002367B91DD30>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
    "# })\n",
    "# | RunnableAssign(mapper={\n",
    "#     answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
    "#               context: RunnableLambda(format_docs)\n",
    "#             }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
    "#             | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n'), additional_kwargs={})])\n",
    "#             | ChatOllama(model='gemma3:1b')\n",
    "#             | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
    "#   }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])\n",
    "\n",
    "# ‚úÖ What's happening?\n",
    "# ‚Ä¢\tNow you're connecting everything:\n",
    "# o\tRetriever finds the most relevant text from the DB.\n",
    "# o\tDocument chain sends it to the LLM.\n",
    "# ‚Ä¢\tretrieval_chain is your final smart pipeline.\n",
    "\n",
    "# üß† 10. Ask a Question and Get Final Answer\n",
    "response = retrieval_chain.invoke({\"input\": \"Key Factors Behind Petrol and Diesel Rates\"})\n",
    "# ‚úÖ What's happening?\n",
    "# ‚Ä¢\tYou ask a question.\n",
    "# ‚Ä¢\tThe system:\n",
    "# 1.\tRetrieves the most relevant documents.\n",
    "# 2.\tPasses them to the LLM with your question.\n",
    "# 3.\tReturns a smart answer.\n",
    "\n",
    "# üîç 11. Check the Answer & Context\n",
    "response['answer']       # The final answer\n",
    "response['context']      # The document(s) used to answer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3077431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£ Load documents (PDFs, text, etc.)\n",
    "# 2Ô∏è‚É£ Split into chunks ‚û°Ô∏è called document chunks\n",
    "# 3Ô∏è‚É£ Convert to embeddings using OpenAI Embeddings\n",
    "# 4Ô∏è‚É£ Store embeddings in Vector DB (like FAISS)\n",
    "# 5Ô∏è‚É£ Query DB via Similarity Search ‚úÖ (basic)\n",
    "# 6Ô∏è‚É£ Create Document Chain to give context to LLM\n",
    "# 7Ô∏è‚É£ Create Retriever ‚û°Ô∏è Automate fetching context\n",
    "# 8Ô∏è‚É£ Combine Retriever + Document Chain ‚û°Ô∏è Retrieval Chain\n",
    "# 9Ô∏è‚É£ Call Retrieval Chain with input ‚û°Ô∏è Get smart answers ‚úÖ\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
