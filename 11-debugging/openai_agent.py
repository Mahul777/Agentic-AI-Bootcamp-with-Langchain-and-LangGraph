# ğŸ”¸ 1. Import Required Libraries
from typing import Annotated
from langchain_core.messages import BaseMessage
from langgraph.graph import StateGraph, END, START
from langgraph.prebuilt import add_messages
# ChatGroq
from langchain_groq import ChatGroq
# TypedDict
from typing import TypedDict
import os
from dotenv import load_dotenv
#âœ… Purpose of each:
# â€¢	StateGraph: For building workflows.
# â€¢	ChatOpenAI: LLM model.
# â€¢	add_messages: Reducer to handle message state.
# â€¢	load_dotenv(): Load .env file for API keys.

load_dotenv()

os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["LANGCHAIN_API_KEY"] = os.getenv("LANGCHAINAPI_KEY")

# ğŸ”¸ 2. Create State Class
class State(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]
# âœ… This State class will:
# â€¢	Store a list of messages ğŸ’¬.
# â€¢	Use add_messages to collect new inputs during chat

# ğŸ”¸ 3. Initialize the Model
model=ChatGroq(model="llama-3.3-70b-versatile")

# ğŸ”¸ 4. Define Graph Function
def make_default_graph():
    workflow = StateGraph(State) 

    def call_model(state):
        return {"messages": model.invoke(state["messages"])}

    workflow.add_node("agent", call_model)
    workflow.set_entry_point("agent")
    workflow.set_finish_point("agent")

    workflow.add_edge(START, "agent")
    workflow.add_edge("agent", END)

    agent = workflow.compile()
    return agent
# âœ… What this does:
# â€¢	ğŸ“Œ Creates a graph workflow.
# â€¢	ğŸ§  Adds a node "agent" that calls the model.
# â€¢	ğŸ” Connects start â†’ agent â†’ end.
# â€¢	âœ… Compiles and returns the agent.

def make_alternate_graph():
    workflow = StateGraph(State)

    def add(a: int, b: int):
        return a + b

    tools = {"add": lambda a, b: add(a, b)}
    tool_node = ToolNode(tools)

    def call_model(state):
        return {"messages": model.invoke(state["messages"])}

    workflow.add_node("tools", tool_node)
    workflow.add_node("agent", call_model)

    workflow.set_entry_point("agent")
    workflow.set_finish_point("agent")

    workflow.add_edge(START, "agent")
    workflow.add_conditional_edges("agent", {
        "continue": END,
        "tool_call": "tools"
    })
    workflow.add_edge("tools", "agent")

    return workflow.compile()
# âœ… This graph has:
# â€¢	ğŸ‘¨â€ğŸ’» A tool: simple add function (a + b)
# â€¢	ğŸ¤– Agent can now call tools like calculator.
# â€¢	ğŸ” Workflow:
# â€¢	START â†’ agent â†’ tool â†’ agent â†’ END

# ğŸ”¸ 5. Call the Function
#agent = make_default_graph()
agent = make_alternate_graph()
# âœ… ğŸ§¾ Create Landgraf Config File (langgraph.json)
# {
#   "dependencies": ["."],
#   "graphs": {
#     "openai_agent": {
#       "entrypoint": "openai_agent.py:agent"
#     }
#   },
#   "env_file": "../.env"
# }
# âœ… Explanation:
# â€¢	"dependencies" â†’ Current folder.
# â€¢	"entrypoint" â†’ File and variable (openai_agent.py:agent).
# â€¢	"env_file" â†’ Path to .env file outside the folder

 
