{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be257a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ 1. Load Environment Variables (like API keys)\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() # load all the environment variables\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# üß± 2. Create an Embedding Model\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "embeddings\n",
    "\n",
    "# üìù 3. Convert a Sample Text into Vector (Embedding)\n",
    "text = \"This is a tutorial on OPENAI embedding\"\n",
    "query_result = embeddings.embed_query(text)\n",
    "query_result\n",
    "\n",
    "# üî¢ 4. Change Embedding Size (Optional)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\", embedding_size=1024)\n",
    "# You create another version of the embedding model with only 1024 dimensions (smaller size).\n",
    "# Useful if you want to save memory or speed things up.\n",
    "\n",
    "# üìÑ 5. Load Your Document (e.g., speech.txt)\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('speech.txt')\n",
    "docs = loader.load()\n",
    "\n",
    "# ‚úÇÔ∏è 6. Split the Document into Chunks\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "final_documents = text_splitter.split_documents(docs)\n",
    "# Big documents are hard to process all at once.\n",
    "# So you split them into smaller pieces (chunks of 500 characters with 50-character overlap).\n",
    "# Now you have smaller pieces to store in a vector DB.\n",
    "\n",
    "# üíæ 7. Store the Documents as Vectors in a Vector Database (Chroma)\n",
    "from langchain_community.vectorstores import Chroma\n",
    "db = Chroma.from_documents(final_documents, embeddings)\n",
    "db\n",
    "# You store the text chunks as vectors using Chroma (a vector database).\n",
    "# Each chunk is now searchable by meaning, not just by exact words.\n",
    "\n",
    "# üîç 8. Query the Vector Store (Semantic Search)\n",
    "query = \"It will be all the easier for us to conduct ourselves as belligerents\"\n",
    "retrieved_results = db.similarity_search(query)\n",
    "print(retrieved_results)\n",
    "# You ask a question or give a query.\n",
    "# The database searches for text chunks that are similar in meaning, not just keywords.\n",
    "# It returns the most relevant parts from your document.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
